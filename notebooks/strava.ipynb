{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b70496e8",
   "metadata": {},
   "source": [
    "# Strava: Guide to data extraction and analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f87e689",
   "metadata": {},
   "source": [
    "<img src='https://i.imgur.com/jneRMpU.jpg' height=\"500\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ae9032",
   "metadata": {},
   "source": [
    "A picture of the Strava Mobile Application\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf65040",
   "metadata": {},
   "source": [
    "Strava is often regarded as the “social network for athletes.” It lets you track your running and riding with GPS, join Challenges, share photos from your activities, and follow friends. <br>\n",
    "\n",
    "Strava follows a fremium model offering a digital service accessible through its mobile applications (iOS and Android). Users also have an option to upgrade and unlock more advanced features like Custom Goals, Training Plans, Race Analysis, etc for a monthly fee of $5-8. <br>\n",
    "\n",
    "We've been using the strava application for the past few weeks and we will show you how to extract its data, visualize your runs and compute correlations between multiple metrics of the data. The Strava API allows the users to extract all sorts of data on athletes, segments, routes, clubs, and gear. However, for this notebook we will be focusing on metrics of the participant's activities like dates, speed, elevation, duration, etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8f20a5",
   "metadata": {},
   "source": [
    "We will be able to extract the following parameters:\n",
    "\n",
    "Parameter Name  | Sampling Frequency \n",
    "-------------------|-----------------\n",
    "<b>Moving Time</b> |  Per Activity \n",
    "<b>Elapsed Time</b> |  Per Activity \n",
    "<b>Average Speed</b> |  Per Activity \n",
    "<b>Maximum Speed</b> |  Per Activity \n",
    "<b>Average Cadence</b> |  Per Activity\n",
    "<b>Maximum Cadence</b> |  Per Activity\n",
    "<b>Average Watts</b> |  Per Activity\n",
    "<b>Maximum Watts</b> |  Per Activity\n",
    "<b>Average Heart Rate</b> |  Per Activity\n",
    "<b>Maximum Heartrate</b> |  Per Activity\n",
    "<b>Distance</b>      |  Records every change in user's position\n",
    "<b>Polyline Summary</b> |  Records every change in user's position\n",
    "<b>Total Elevation Gain</b> |  Sampling Frequency depends upon user's fitness tracker\n",
    "<b>Heart Rate</b>  | Sampling Frequency depends upon user's fitness tracker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ebad13",
   "metadata": {},
   "source": [
    "** All of these are individual paramaters that can be directly extracted using wearipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03f1a22",
   "metadata": {},
   "source": [
    "In this guide, we sequentially cover the following **five** topics to extract data from Cronometer servers:\n",
    "\n",
    "1. **Set up**<br>\n",
    "2. **Authentication/Authorization**<br>\n",
    "   - Requires only client_id, client_secret and refresh_token.<br>\n",
    "3. **Data extraction**<br>\n",
    "  - We get data via wearipedia in a couple lines of code<br>\n",
    "4. **Data Exporting**\n",
    "    - We export all of this data to file formats compatible by R, Excel, and MatLab.\n",
    "5. **Adherence**\n",
    "    - We simulate non-adherence by dynamically removing datapoints from our simulated data.\n",
    "6. **Visualization**\n",
    "    - We create a simple plot to visualize our data.\n",
    "7. **Advanced visualization**\n",
    "    - 7.1 Visualizing participant's Overall Activity!<br>\n",
    "    - 7.2 Visualizing participant's Weekly Summary!<br>\n",
    "    - 7.3 Visualizing Participant's Runs!<br>\n",
    "8. **Statistical Data Analysis** <br>\n",
    "  - 8.1  Analyzing correlation between Participant's data! <br>\n",
    "9. **Outlier Detection and Data Cleaning** <br>\n",
    "  - 9.1 Highlighting Outliers!\n",
    "\n",
    "Disclaimer: this notebook is purely for educational purposes. All of the data currently stored in this notebook is purely *synthetic*, meaning randomly generated according to rules we created. Despite this, the end-to-end data extraction pipeline has been tested on our own data, meaning that if you enter your own email and password on your own Colab instance, you can visualize your own *real* data. That being said, we were unable to thoroughly test the timezone functionality, though, since we only have one account, so beware."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675c5b05",
   "metadata": {},
   "source": [
    "# 1. Setup\n",
    "\n",
    "## Participant Setup\n",
    "\n",
    "Dear Participant,\n",
    "\n",
    "Once you download the strava app, please set it up by following these resources:\n",
    "- Written guide: https://www.runnersworld.com/beginner/g25619156/what-is-strava/\n",
    "- Video guide: https://www.youtube.com/watch?v=LHtCxdrZFJ8&ab_channel=RunWithJ\n",
    "\n",
    "Make sure that your phone is logged to the strava app using the Strava login credentials (email and password) given to you by the data receiver.\n",
    "\n",
    "Best,\n",
    "\n",
    "Wearipedia\n",
    "\n",
    "## Data Receiver Setup\n",
    "\n",
    "Please follow the below steps:\n",
    "\n",
    "1. Create an email address for the participant, for example `foo@email.com`.\n",
    "2. Create a Strava account with the email `foo@email.com` and some random password.\n",
    "3. Keep `foo@email.com` and password stored somewhere safe.\n",
    "4. Distribute the device to the participant and instruct them to follow the participant setup letter above.\n",
    "5. Next, go to https://developers.strava.com/\n",
    "6. Click on \"Create and Manage your App\"\n",
    "<img src='https://i.imgur.com/nBdQDR6.png' width=\"850\" height=\"500\">\n",
    "7. Strava will prompt you to login. Make sure to login with the account that participants' credentials\n",
    "8. Fill out your details:<br>\n",
    "   Here's an example with some dummy information! <br>\n",
    "   <img src='https://i.imgur.com/O9jRXQQ.png' width=\"450\" height=\"400\">\n",
    "9. Next, Strava will ask you to upload a app icon. I personally uploaded the Strava logo in the option. <br>\n",
    "    <img src='https://i.imgur.com/wQHGyL2.png' width=\"600\" height=\"400\"> <br> \n",
    "   Copy and crop the Image to your preference and hit save! <br>\n",
    "    <img src='https://i.imgur.com/kKodoZm.png' width=\"600\" height=\"400\">\n",
    "10. <img src='https://i.imgur.com/gV2tGlK.png' width=\"600\" height=\"400\">\n",
    "    \n",
    "    Copy and paste your your client ID and Client Token in the box below and give it to the researcher.\n",
    "    There is an access token and refresh token on the profile page, but it does not have the scope that we want.\n",
    "11. Modify the link below with your client ID and navigate to\n",
    "\n",
    "    \"https://www.strava.com/oauth/authorize?client_id={YOUR_CLIENT_ID}&response_type=code&redirect_uri=http://localhost/exchange_token&approval_prompt=force&scope=read_all,profile:read_all,activity:read_all\"\n",
    "\n",
    "    You should see a page asking you to authorize your app with the following scopes:\n",
    "    <img src='https://i.imgur.com/oEugtqT.png' width=\"600\" height=\"400\">\n",
    "\n",
    "    Click authorize. This will redirect you to a \"This site can't be reached page\". This is expected, but copy the \"code\" parameter out of the URL. For example:\n",
    "\n",
    "    http://localhost/exchange_token?state=&code=13ced876511c89c9c3fb00e8d8898eb92e6102a2&scope=read,activity:read_all,profile:read_all,read_all -> \"13ced876511c89c9c3fb00e8d8898eb92e6102a2\"\n",
    "\n",
    "    Give this to the researcher as well.\n",
    "    \n",
    "13. Install the `wearipedia` Python package to easily extract data from this device via the Strava API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f0aab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wearipedia\n",
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9f55a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wearipedia\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe816960",
   "metadata": {},
   "source": [
    "# 2. Authentication and Authorization\n",
    "\n",
    "To obtain access to data, authorization is required. Put in your client id, client secret token and refresh token for your Strava account. We'll use this to extract the data in the sections below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08fbee4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Enter Strava API credentials\n",
    "client_id = \"\" #@param {type:\"string\"}\n",
    "client_secret = \"\" #@param {type:\"string\"}\n",
    "code = \"\" #@param {type:\"string\"}\n",
    "\n",
    "def get_strava_refresh_token(client_id, client_secret, code):\n",
    "    \"\"\"\n",
    "    Get a refresh token from Strava API using authorization code.\n",
    "    \n",
    "    Args:\n",
    "        client_id (str): Your Strava API client ID\n",
    "        client_secret (str): Your Strava API client secret\n",
    "        code (str): Authorization code received from Strava\n",
    "        \n",
    "    Returns:\n",
    "        str: The refresh token\n",
    "    \"\"\"\n",
    "    url = \"https://www.strava.com/api/v3/oauth/token\"\n",
    "    payload = {\n",
    "        'client_id': client_id,\n",
    "        'client_secret': client_secret,\n",
    "        'code': code,\n",
    "        'grant_type': 'authorization_code'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(url, data=payload)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        refresh_token = data.get('refresh_token')\n",
    "        \n",
    "        if not refresh_token:\n",
    "            print(\"Warning: No refresh token found in response\")\n",
    "        return refresh_token\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error making request: {e}\")\n",
    "        return None\n",
    "        \n",
    "# If you have a refresh token already directly, you can fill this in instead of calling the above function.\n",
    "refresh_token = get_strava_refresh_token(client_id, client_secret, code)\n",
    "print(\"Your refresh_token is\", refresh_token)\n",
    "\n",
    "# Note that the authorization code can only be used once, so you should hold onto this refresh token, or you can go through the authorize page "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fec1a3",
   "metadata": {},
   "source": [
    "# 3. Data Extraction\n",
    "\n",
    "Data can be extracted via [wearipedia](https://github.com/Stanford-Health/wearipedia/), our open-source Python package that unifies dozens of complex wearable device APIs into one simple, common interface.\n",
    "\n",
    "First, we'll set a date range and then extract all of the data within that date range. You can select whether you would like synthetic data or not with the checkbox."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97175015",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Enter start and end dates (in the format yyyy-mm-dd)\n",
    "\n",
    "#set start and end dates - this will give you all the data from 2000-01-01 (January 1st, 2000) to 2100-02-03 (February 3rd, 2100), for example\n",
    "start_date='2022-03-01' #@param {type:\"string\"}\n",
    "end_date='2022-06-17' #@param {type:\"string\"}\n",
    "synthetic = True #@param {type:\"boolean\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd01256",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = wearipedia.get_device(\"strava/strava\")\n",
    "\n",
    "if not synthetic:\n",
    "    device.authenticate({\n",
    "    'client_id':client_id,\n",
    "    'client_secret':client_secret,\n",
    "    'refresh_token':refresh_token\n",
    "    })\n",
    "\n",
    "params = {\"start_date\": start_date, \"end_date\": end_date}\n",
    "\n",
    "distance = device.get_data(\"distance\", params=params)\n",
    "moving_time = device.get_data(\"moving_time\", params=params)\n",
    "elapsed_time = device.get_data(\"elapsed_time\", params=params)\n",
    "total_elevation_gain = device.get_data(\"total_elevation_gain\", params=params)\n",
    "average_speed = device.get_data(\"average_speed\", params=params)\n",
    "max_speed = device.get_data(\"max_speed\", params=params)\n",
    "average_heartrate = device.get_data(\"average_heartrate\", params=params)\n",
    "max_heartrate = device.get_data(\"max_heartrate\", params=params)\n",
    "map_summary_polyline = device.get_data(\"map_summary_polyline\", params=params)\n",
    "elev_high = device.get_data(\"elev_high\", params=params)\n",
    "elev_low = device.get_data(\"elev_low\", params=params)\n",
    "average_cadence = device.get_data(\"average_cadence\", params=params)\n",
    "average_watts = device.get_data(\"average_watts\", params=params)\n",
    "kilojoules = device.get_data(\"kilojoules\", params=params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bbc267",
   "metadata": {},
   "source": [
    "# 4. Data Exporting\n",
    "\n",
    "In this section, we export all of this data to formats compatible with popular scientific computing software (R, Excel, Google Sheets, Matlab). Specifically, we will first export to JSON, which can be read by R and Matlab. Then, we will export to CSV, which can be consumed by Excel, Google Sheets, and every other popular programming language.\n",
    "\n",
    "## Exporting to JSON (R, Matlab, etc.)\n",
    "\n",
    "Exporting to JSON is fairly simple. We export each datatype separately and also export a complete version that includes all simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ab6600",
   "metadata": {},
   "outputs": [],
   "source": [
    "def datacleanup(data):\n",
    "    for d in data:\n",
    "        d['start_date'] = str(d['start_date'])\n",
    "    return data\n",
    "\n",
    "json.dump(datacleanup(distance), open(\"distance.json\", \"w\"))\n",
    "json.dump(datacleanup(moving_time), open(\"moving_time.json\", \"w\"))\n",
    "json.dump(datacleanup(elapsed_time), open(\"elapsed_time.json\", \"w\"))\n",
    "json.dump(datacleanup(total_elevation_gain), open(\"total_elevation_gain.json\", \"w\"))\n",
    "json.dump(datacleanup(average_speed), open(\"average_speed.json\", \"w\"))\n",
    "json.dump(datacleanup(max_speed), open(\"max_speed.json\", \"w\"))\n",
    "json.dump(datacleanup(average_heartrate), open(\"average_heartrate.json\", \"w\"))\n",
    "json.dump(datacleanup(max_heartrate), open(\"max_heartrate.json\", \"w\"))\n",
    "json.dump(datacleanup(map_summary_polyline), open(\"map_summary_polyline.json\", \"w\"))\n",
    "json.dump(datacleanup(elev_high), open(\"elev_high.json\", \"w\"))\n",
    "json.dump(datacleanup(elev_low), open(\"elev_low.json\", \"w\"))\n",
    "json.dump(datacleanup(average_cadence), open(\"average_cadence.json\", \"w\"))\n",
    "json.dump(datacleanup(average_watts), open(\"average_watts.json\", \"w\"))\n",
    "json.dump(datacleanup(kilojoules), open(\"kilojoules.json\", \"w\"))\n",
    "\n",
    "\n",
    "complete = {\n",
    "    \"distance\": distance,\n",
    "    'moving_time':moving_time,\n",
    "    'elapsed_time':elapsed_time,\n",
    "    'total_elevation_gain':total_elevation_gain,\n",
    "    'average_speed':average_speed,\n",
    "    'max_speed':max_speed,\n",
    "    'average_heartrate':average_heartrate,\n",
    "    'max_heartrate':max_heartrate,\n",
    "    'map_summary_polyline':map_summary_polyline,\n",
    "    'elev_high':elev_high,\n",
    "    'elev_low':elev_low,\n",
    "    'average_cadence':average_cadence,\n",
    "    'average_watts':average_watts,\n",
    "    'kilojoules':kilojoules\n",
    "}\n",
    "\n",
    "json.dump(complete, open(\"complete.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a25f19",
   "metadata": {},
   "source": [
    "Feel free to open the file viewer (see left pane) to look at the outputs!\n",
    "\n",
    "## Exporting to CSV and XLSX (Excel, Google Sheets, R, Matlab, etc.)\n",
    "\n",
    "Exporting to CSV/XLSX requires a bit more processing, since they enforce a pretty restrictive schema.\n",
    "\n",
    "We will thus export steps, heart rates, and breath rates all as separate files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169bb66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_df = pd.DataFrame.from_dict(complete['distance'])\n",
    "distance_df.to_csv('distance.csv')\n",
    "distance_df.to_excel('distance.xlsx')\n",
    "\n",
    "moving_time_df = pd.DataFrame.from_dict(complete['moving_time'])\n",
    "moving_time_df.to_csv('moving_time.csv')\n",
    "moving_time_df.to_excel('moving_time.xlsx')\n",
    "\n",
    "elapsed_time_df = pd.DataFrame.from_dict(complete['elapsed_time'])\n",
    "elapsed_time_df.to_csv('elapsed_time.csv')\n",
    "elapsed_time_df.to_excel('elapsed_time.xlsx')\n",
    "\n",
    "total_elevation_gain_df = pd.DataFrame.from_dict(complete['total_elevation_gain'])\n",
    "total_elevation_gain_df.to_csv('total_elevation_gain.csv')\n",
    "total_elevation_gain_df.to_excel('total_elevation_gain.xlsx')\n",
    "\n",
    "average_speed_df = pd.DataFrame.from_dict(complete['average_speed'])\n",
    "average_speed_df.to_csv('average_speed.csv')\n",
    "average_speed_df.to_excel('average_speed.xlsx')\n",
    "\n",
    "max_speed_df = pd.DataFrame.from_dict(complete['max_speed'])\n",
    "max_speed_df.to_csv('max_speed.csv')\n",
    "max_speed_df.to_excel('max_speed.xlsx')\n",
    "\n",
    "average_heartrate_df = pd.DataFrame.from_dict(complete['average_heartrate'])\n",
    "average_heartrate_df.to_csv('average_heartrate.csv')\n",
    "average_heartrate_df.to_excel('average_heartrate.xlsx')\n",
    "\n",
    "max_heartrate_df = pd.DataFrame.from_dict(complete['max_heartrate'])\n",
    "max_heartrate_df.to_csv('max_heartrate.csv')\n",
    "max_heartrate_df.to_excel('max_heartrate.xlsx')\n",
    "\n",
    "map_summary_polyline_df = pd.DataFrame.from_dict(complete['map_summary_polyline'])\n",
    "map_summary_polyline_df.to_csv('map_summary_polyline.csv')\n",
    "map_summary_polyline_df.to_excel('map_summary_polyline.xlsx')\n",
    "\n",
    "elev_high_df = pd.DataFrame.from_dict(complete['elev_high'])\n",
    "elev_high_df.to_csv('elev_high.csv')\n",
    "elev_high_df.to_excel('elev_high.xlsx')\n",
    "\n",
    "elev_low_df = pd.DataFrame.from_dict(complete['elev_low'])\n",
    "elev_low_df.to_csv('elev_low.csv')\n",
    "elev_low_df.to_excel('elev_low.xlsx')\n",
    "\n",
    "average_cadence_df = pd.DataFrame.from_dict(complete['average_cadence'])\n",
    "average_cadence_df.to_csv('average_cadence.csv')\n",
    "average_cadence_df.to_excel('average_cadence.xlsx')\n",
    "\n",
    "average_watts_df = pd.DataFrame.from_dict(complete['average_watts'])\n",
    "average_watts_df.to_csv('average_watts.csv')\n",
    "average_watts_df.to_excel('average_watts.xlsx')\n",
    "\n",
    "kilojoules_df = pd.DataFrame.from_dict(complete['kilojoules'])\n",
    "kilojoules_df.to_csv('kilojoules.csv')\n",
    "kilojoules_df.to_excel('kilojoules.xlsx')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b30043",
   "metadata": {},
   "source": [
    "Again, feel free to look at the output files and download them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a7b240",
   "metadata": {},
   "source": [
    "# 5. Adherence\n",
    "\n",
    "The device simulator already automatically randomly deletes small chunks of the day. In this section, we will simulate non-adherence over longer periods of time from the participant (day-level and week-level).\n",
    "\n",
    "Then, we will detect this non-adherence and give a Pandas DataFrame that concisely describes when the participant has had their device on and off throughout the entirety of the time period, allowing you to calculate how long they've had it on/off etc.\n",
    "\n",
    "We will first delete a certain % of blocks either at the day level or week level, with user input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350a67b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Non-adherence simulation\n",
    "block_level = \"day\" #@param [\"day\", \"week\"]\n",
    "adherence_percent = 0.89 #@param {type:\"slider\", min:0, max:1, step:0.01}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857c9908",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete = {\n",
    "    \"distance\": distance,\n",
    "    'moving_time':moving_time,\n",
    "    'elapsed_time':elapsed_time,\n",
    "    'total_elevation_gain':total_elevation_gain,\n",
    "    'average_speed':average_speed,\n",
    "    'max_speed':max_speed,\n",
    "    'average_heartrate':average_heartrate,\n",
    "    'max_heartrate':max_heartrate,\n",
    "    'map_summary_polyline':map_summary_polyline,\n",
    "    'elev_high':elev_high,\n",
    "    'elev_low':elev_low,\n",
    "    'average_cadence':average_cadence,\n",
    "    'average_watts':average_watts,\n",
    "    'kilojoules':kilojoules\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c189e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "if block_level == \"day\":\n",
    "    block_length = 1\n",
    "elif block_level == \"week\":\n",
    "    block_length = 7\n",
    "\n",
    "\n",
    "\n",
    "# This function will randomly remove datapoints from the \n",
    "# data we have recieved from Cronometer based on the\n",
    "# adherence_percent\n",
    "\n",
    "def AdherenceSimulator(data):\n",
    "\n",
    "  num_blocks = len(data) // block_length\n",
    "  num_blocks_to_keep = int(adherence_percent * num_blocks)\n",
    "  idxes = np.random.choice(np.arange(num_blocks), replace=False, \n",
    "  size=num_blocks_to_keep)\n",
    "\n",
    "  adhered_data = []\n",
    "\n",
    "  for i in range(len(data)):\n",
    "      if i in idxes:\n",
    "          start = i * block_length\n",
    "          end = (i + 1) * block_length\n",
    "          for j in range(i,i+1):\n",
    "            adhered_data.append(data[j])\n",
    "  \n",
    "  return adhered_data\n",
    "\n",
    "\n",
    "# Adding adherence for distance\n",
    "\n",
    "distance = AdherenceSimulator(distance)\n",
    "\n",
    "# Adding adherence for moving_time\n",
    "\n",
    "moving_time = AdherenceSimulator(moving_time)\n",
    "\n",
    "# Adding adherence for elapsed_time\n",
    "\n",
    "elapsed_time = AdherenceSimulator(elapsed_time)\n",
    "\n",
    "# Adding adherence for total_elevation_gain\n",
    "\n",
    "total_elevation_gain = AdherenceSimulator(total_elevation_gain)\n",
    "\n",
    "# Adding adherence for average_speed\n",
    "\n",
    "average_speed = AdherenceSimulator(average_speed)\n",
    "\n",
    "# Adding adherence for max_speed\n",
    "\n",
    "max_speed = AdherenceSimulator(max_speed)\n",
    "\n",
    "# Adding adherence for average_heartrate\n",
    "\n",
    "average_heartrate = AdherenceSimulator(average_heartrate)\n",
    "\n",
    "# Adding adherence for max_heartrate\n",
    "\n",
    "max_heartrate = AdherenceSimulator(max_heartrate)\n",
    "\n",
    "# Adding adherence for map_summary_polyline\n",
    "\n",
    "map_summary_polyline = AdherenceSimulator(map_summary_polyline)\n",
    "\n",
    "# Adding adherence for elev_high\n",
    "\n",
    "elev_high = AdherenceSimulator(elev_high)\n",
    "\n",
    "# Adding adherence for elev_low\n",
    "\n",
    "elev_low = AdherenceSimulator(elev_low)\n",
    "\n",
    "# Adding adherence for average_cadence\n",
    "\n",
    "average_cadence = AdherenceSimulator(average_cadence)\n",
    "\n",
    "# Adding adherence for average_watts\n",
    "\n",
    "average_watts = AdherenceSimulator(average_watts)\n",
    "\n",
    "# Adding adherence for kilojoules\n",
    "\n",
    "kilojoules = AdherenceSimulator(kilojoules)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c617a06c",
   "metadata": {},
   "source": [
    "And now we have significantly fewer datapoints! This will give us a more realistic situation, where participants may take off their device for days or weeks at a time.\n",
    "\n",
    "Now let's detect non-adherence. We will return a Pandas DataFrame sampled at every day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f1ddb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_df = pd.DataFrame.from_dict(distance)\n",
    "moving_time_df = pd.DataFrame.from_dict(moving_time)\n",
    "elapsed_time_df = pd.DataFrame.from_dict(elapsed_time)\n",
    "total_elevation_gain_df = pd.DataFrame.from_dict(total_elevation_gain)\n",
    "average_speed_df = pd.DataFrame.from_dict(average_speed)\n",
    "max_speed_df = pd.DataFrame.from_dict(max_speed)\n",
    "average_heartrate_df = pd.DataFrame.from_dict(average_heartrate)\n",
    "max_heartrate_df = pd.DataFrame.from_dict(max_heartrate)\n",
    "map_summary_polyline_df = pd.DataFrame.from_dict(map_summary_polyline)\n",
    "elev_high_df = pd.DataFrame.from_dict(elev_high)\n",
    "elev_low_df = pd.DataFrame.from_dict(elev_low)\n",
    "average_cadence_df = pd.DataFrame.from_dict(average_cadence)\n",
    "average_watts_df = pd.DataFrame.from_dict(average_watts)\n",
    "kilojoules_df = pd.DataFrame.from_dict(kilojoules)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ee7ad7",
   "metadata": {},
   "source": [
    "We can plot this out, and we get adherence at one-day frequency throughout the entirety of the data collection period. For this chart we will plot Distance Traveled per day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4153da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_df_daily = distance_df.assign(start_date = distance_df.get('start_date').apply(lambda x: x[:10]))\n",
    "distance_df_daily = distance_df_daily.groupby('start_date').sum(numeric_only=True)\n",
    "\n",
    "dates = pd.date_range(start_date,end_date)\n",
    "\n",
    "energy = []\n",
    "\n",
    "for d in dates:\n",
    "    res = distance_df_daily[distance_df_daily.index == datetime.datetime.strftime(d,\n",
    "    '%Y-%m-%d')]['distance']\n",
    "    if len(res) == 0:\n",
    "        energy.append(None)\n",
    "    else:\n",
    "        energy.append(res.iloc[0])\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.set(style=\"whitegrid\")\n",
    "sns.lineplot(x=dates, y=energy)\n",
    "plt.ylabel('Distance traveled (m)')\n",
    "plt.xlabel('Date')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f206e8",
   "metadata": {},
   "source": [
    "# 6. Visualization\n",
    "\n",
    "We've extracted lots of data, but what does it look like?\n",
    "\n",
    "In this section, we will be visualizing our three kinds of data in a simple, customizable plot! This plot is intended to provide a starter example for plotting, whereas later examples emphasize deep control and aesthetics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5870d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Basic Plot\n",
    "feature = \"total_elevation_gain\" #@param ['moving_time', 'elapsed_time','total_elevation_gain','average_speed']\n",
    "start_date = \"2022-03-04\" #@param {type:\"date\"}\n",
    "time_interval = \"full time\" #@param [\"one week\", \"full time\"]\n",
    "smoothness = 0.02 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
    "smooth_plot = True #@param {type:\"boolean\"}\n",
    "\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "start_date = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "\n",
    "if time_interval == \"one week\":\n",
    "    day_idxes = [i for i,d in enumerate(dates) if d >= start_date and d <= start_date + timedelta(days=7)]\n",
    "    end_date = start_date + timedelta(days=7)\n",
    "elif time_interval == \"full time\":\n",
    "    day_idxes = [i for i,d in enumerate(dates) if d >= start_date]\n",
    "    end_date = dates[-1]\n",
    "\n",
    "if feature == \"moving_time\":\n",
    "    moving_time_daily = moving_time_df.assign(start_date = moving_time_df.get('start_date').apply(lambda x: x[:10]))\n",
    "    moving_time_daily = moving_time_daily.groupby('start_date').sum(numeric_only=True)\n",
    "    concat_moving_time = []\n",
    "    for i,d in enumerate(dates):\n",
    "        day = d.strftime('%Y-%m-%d')\n",
    "        if i in day_idxes:\n",
    "            mt = moving_time_daily[moving_time_daily.index==day]\n",
    "            if len(mt) != 0:\n",
    "                concat_moving_time += [(day,mt.iloc[0].moving_time)]\n",
    "            else:\n",
    "                concat_moving_time += [(day,None)]\n",
    "    ts = [x[0] for x in concat_moving_time]\n",
    "\n",
    "    day_arr = [x[1] for x in concat_moving_time]\n",
    "\n",
    "    sigma = 200 * smoothness\n",
    "\n",
    "    title_fillin = \"Moving Time\"\n",
    "\n",
    "if feature == \"elapsed_time\":\n",
    "    elapsed_time_daily = elapsed_time_df.assign(start_date = elapsed_time_df.get('start_date').apply(lambda x: x[:10]))\n",
    "    elapsed_time_daily = elapsed_time_daily.groupby('start_date').sum(numeric_only=True)\n",
    "    concat_elapsed_time = []\n",
    "    for i,d in enumerate(dates):\n",
    "        day = d.strftime('%Y-%m-%d')\n",
    "        if i in day_idxes:\n",
    "            et = elapsed_time_daily[elapsed_time_daily.index==day]\n",
    "            if len(et) != 0:\n",
    "                concat_elapsed_time += [(day,et.iloc[0].elapsed_time)]\n",
    "            else:\n",
    "                concat_elapsed_time += [(day,None)]\n",
    "    ts = [x[0] for x in concat_elapsed_time]\n",
    "\n",
    "    day_arr = [x[1] for x in concat_elapsed_time]\n",
    "\n",
    "    sigma = 200 * smoothness\n",
    "\n",
    "    title_fillin = \"Elapsed Time\"\n",
    "\n",
    "if feature == \"total_elevation_gain\":\n",
    "    total_elevation_gain_daily = total_elevation_gain_df.assign(start_date = total_elevation_gain_df.get('start_date').apply(lambda x: x[:10]))\n",
    "    total_elevation_gain_daily = total_elevation_gain_daily.groupby('start_date').sum(numeric_only=True)\n",
    "    concat_total_elevation_gain = []\n",
    "    for i,d in enumerate(dates):\n",
    "        day = d.strftime('%Y-%m-%d')\n",
    "        if i in day_idxes:\n",
    "            et = total_elevation_gain_daily[total_elevation_gain_daily.index==day]\n",
    "            if len(et) != 0:\n",
    "                concat_total_elevation_gain += [(day,et.iloc[0].total_elevation_gain)]\n",
    "            else:\n",
    "                concat_total_elevation_gain += [(day,None)]\n",
    "    ts = [x[0] for x in concat_total_elevation_gain]\n",
    "\n",
    "    day_arr = [x[1] for x in concat_total_elevation_gain]\n",
    "\n",
    "    sigma = 200 * smoothness\n",
    "\n",
    "    title_fillin = \"Total Elevation Gain\"\n",
    "\n",
    "if feature == \"average_speed\":\n",
    "    average_speed_daily = average_speed_df.assign(start_date = average_speed_df.get('start_date').apply(lambda x: x[:10]))\n",
    "    average_speed_daily = average_speed_daily.groupby('start_date').sum(numeric_only=True)\n",
    "    concat_average_speed = []\n",
    "    for i,d in enumerate(dates):\n",
    "        day = d.strftime('%Y-%m-%d')\n",
    "        if i in day_idxes:\n",
    "            et = average_speed_daily[average_speed_daily.index==day]\n",
    "            if len(et) != 0:\n",
    "                concat_average_speed += [(day,et.iloc[0].average_speed)]\n",
    "            else:\n",
    "                concat_average_speed += [(day,None)]\n",
    "    ts = [x[0] for x in concat_average_speed]\n",
    "\n",
    "    day_arr = [x[1] for x in concat_average_speed]\n",
    "\n",
    "    sigma = 200 * smoothness\n",
    "\n",
    "    title_fillin = \"Average Speed\"\n",
    "    \n",
    "with plt.style.context('ggplot'):\n",
    "    fig, ax = plt.subplots(figsize=(15, 8))\n",
    "\n",
    "    if smooth_plot:\n",
    "        def to_numpy(day_arr):\n",
    "            arr_nonone = [x for x in day_arr if x is not None]\n",
    "            mean_val = int(np.mean(arr_nonone))\n",
    "            for i,x in enumerate(day_arr):\n",
    "                if x is None:\n",
    "                    day_arr[i] = mean_val\n",
    "\n",
    "            return np.array(day_arr)\n",
    "\n",
    "        none_idxes = [i for i,x in enumerate(day_arr) if x is None]\n",
    "        day_arr = to_numpy(day_arr)\n",
    "        from scipy.ndimage import gaussian_filter\n",
    "        day_arr = list(gaussian_filter(day_arr, sigma=sigma))\n",
    "        for i, x in enumerate(day_arr):\n",
    "            if i in none_idxes:\n",
    "                day_arr[i] = None\n",
    "\n",
    "    plt.plot(ts, day_arr)\n",
    "    start_date_str = start_date.strftime('%Y-%m-%d')\n",
    "    end_date_str = end_date.strftime('%Y-%m-%d')\n",
    "    plt.title(f\"{title_fillin} from {start_date_str} to {end_date_str}\",\n",
    "              fontsize=20)\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.xticks(ts[::int(len(ts)/8)])\n",
    "    plt.ylabel(title_fillin)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb3667d",
   "metadata": {},
   "source": [
    "This plot allows you to quickly scan your data at many different time scales (week and full) and for different kinds of measurements (heart rate and weight), which enables easy and fast data exploration.\n",
    "\n",
    "Furthermore, the smoothness parameter makes it easy to look for patterns in long-term trends."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3da7f9",
   "metadata": {},
   "source": [
    "# 7. Advanced Visualization\n",
    "\n",
    "Now we'll do some more advanced plotting that at times features hardcore matplotlib hacking with the benefit of aesthetic quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b4f134",
   "metadata": {},
   "source": [
    "## 7.1 Visualizing participant's Overall Activity!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645c3133",
   "metadata": {},
   "source": [
    "Whenever our participant is curious and logs into Strava to check their overall summary, the Strava app would present their data in the form of a barchart. It should look something similar to this: <br>\n",
    "<img src=\"https://i.imgur.com/cr5VYpu.png\" height=\"300\">\n",
    " <br>\n",
    "<i>Above is a plot from the app! </i><br> <br>\n",
    "\n",
    "Now that we have user's data, let's try to recreate the chart above using our python skills!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925077c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "elevation_df = total_elevation_gain_df.assign(total_elevation_gain \n",
    "  = total_elevation_gain_df['total_elevation_gain'].apply(lambda elevation: elevation*3.281))\n",
    "elevation_df = elevation_df.merge(distance_df.get(['distance','start_date']),on='start_date')\n",
    "elevation_df = elevation_df.merge(moving_time_df.get(['moving_time','start_date']),on='start_date')\n",
    "elevation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dcef62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta, date\n",
    "import seaborn as sns\n",
    "\n",
    "#@title Set date range for the chart above\n",
    "\n",
    "start = \"2022-04-25\" #@param {type:\"date\"}\n",
    "end = \"2022-05-25\" #@param {type:\"date\"}\n",
    "\n",
    "#Converting all the elevation gains from metres to feet\n",
    "elevation_df = total_elevation_gain_df.assign(total_elevation_gain \n",
    "  = total_elevation_gain_df['total_elevation_gain'].apply(lambda elevation: elevation*3.281))\n",
    "elevation_df = elevation_df.merge(distance_df.get(['distance','start_date']),on='start_date')\n",
    "elevation_df = elevation_df.merge(moving_time_df.get(['moving_time','start_date']),on='start_date')\n",
    "\n",
    "#Function that converts data into a date time object\n",
    "datefixer = lambda date: datetime.fromisoformat(date[0:10])\n",
    "\n",
    "#Applying datefixer function to every column of elevation_df\n",
    "elevation_df = elevation_df.assign(date \n",
    "                          = elevation_df.get('start_date').apply(datefixer))\n",
    "\n",
    "#Dictionary to store dates froms start to end date along with elevations\n",
    "date_etime = {}\n",
    "\n",
    "#Starting date of our chart\n",
    "start_date = date(int(start.split('-')[0]),int(start.split('-')[1]),\n",
    "                  int(start.split('-')[2]))\n",
    "\n",
    "#Ending date of our chart\n",
    "end_date = date(int(end.split('-')[0]),int(end.split('-')[1]),\n",
    "                int(end.split('-')[2]))\n",
    "\n",
    "#A list of all dates between start and end date\n",
    "dates = list(pd.date_range(start_date,end_date-timedelta(days=1),freq='d'))\n",
    "\n",
    "# Storing toal distance, time and elevation\n",
    "total_distance = 0\n",
    "total_time = 0\n",
    "total_elevation = 0\n",
    "\n",
    "#Loop to find the elevation for each date within the Data Frame\n",
    "for date_val in dates:\n",
    "  #Initializes the current date in the dictionary as zero\n",
    "  date_etime[str(date_val.day)+\" \"+str(date_val.month_name())[:3]] = 0\n",
    "  for actvity_index in range(len(elevation_df)):\n",
    "    #Checks if the current date is in the activity DataFrame\n",
    "      if(date_val == elevation_df.iloc[actvity_index].get('date')):\n",
    "        #Storing total time, distance and elevation\n",
    "        total_distance = (total_distance \n",
    "        + elevation_df.iloc[actvity_index].get('distance'))\n",
    "        total_time = (total_time +\n",
    "               elevation_df.iloc[actvity_index].get('moving_time'))\n",
    "        total_elevation = (total_elevation +\n",
    "               elevation_df.iloc[actvity_index].get('total_elevation_gain'))\n",
    "        #Stores the elevation of current date in dictionary\n",
    "        date_etime[(str(date_val.day)+\" \"+\n",
    "                  str(date_val.month_name())[:3])] = (date_etime[\n",
    "                  str(date_val.day)+\" \"+str(date_val.month_name())[:3]] +\n",
    "                  elevation_df.iloc[actvity_index].get('total_elevation_gain'))\n",
    "#Resetting seaborn to prevent interference with matplotlib plots\n",
    "sns.reset_orig()\n",
    "\n",
    "\n",
    "# custom font\n",
    "# https://stackoverflow.com/questions/35668219/how-to-set-up-a-custom-font-with-custom-path-to-matplotlib-global-font\n",
    "# download the font and unzip (quiet so it does not print)\n",
    "!wget -q 'https://dl.dafont.com/dl/?f=mustica_pro'\n",
    "!unzip -qo \"index.html?f=mustica_pro\"\n",
    "\n",
    "try:\n",
    "    # move to directory where fonts should be kept\n",
    "    !mv -f MusticaPro-SemiBold.otf\n",
    "\n",
    "    # build cache, redirect to /dev/null to suppress stdout output\n",
    "    !fc-cache -f -v > /dev/null\n",
    "except:\n",
    "    pass\n",
    "    \n",
    "import matplotlib as mpl\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "# try and except, just in case something fails we fallback onto the\n",
    "# default font\n",
    "try:\n",
    "    fe = fm.FontEntry(\n",
    "        #font name\n",
    "        fname='MusticaPro-SemiBold',\n",
    "        name='DejaVu Sans')\n",
    "    fm.fontManager.ttflist.insert(0, fe) # or append is fine\n",
    "    mpl.rcParams['font.family'] = fe.name # = 'your custom ttf font name'\n",
    "except:\n",
    "    pass\n",
    "\n",
    "#Creating a matplotlib plot of size 16,4\n",
    "plt1 = plt.figure(figsize=(16,4))\n",
    "ax = plt1.gca()\n",
    "\n",
    "#Plotting a bar chart with our data in the dictionary\n",
    "plt.bar(date_etime.keys(),date_etime.values(),color=\"#0098DB\", width=0.9)\n",
    "\n",
    "#Adding the title to the chart\n",
    "plt.title( \"$\\\\bf{\"+str(round(total_distance / 1609,1))+\"}$mi  |  \"+\n",
    "          \"$\\\\bf{\"+str(round(total_time/3600))+\"}$h \"+\"$\\\\bf{\"+\n",
    "          str(round(total_time%60))+\"}$m  |  \"\n",
    "          +\"$\\\\bf{\"+str(round(total_elevation))+\"}$ft\")\n",
    "\n",
    "#Only showing xticks for one day/week\n",
    "ax.set_xticks(ax.get_xticks()[::7])\n",
    "\n",
    "#Limiting the y-axis based on our reference chart\n",
    "plt.ylim((0,350))\n",
    "plt.ylabel(\"Total Elevation Gain (Feet)\",color=\"#a1a1a1\")\n",
    "\n",
    "# Removing the spines on top, left and right\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "\n",
    "ax.tick_params(left=False, bottom=True)\n",
    "\n",
    "#Setting the bottom spine to be the same color as the reference chart\n",
    "ax.spines['bottom'].set_color('#0098DB')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c96606",
   "metadata": {},
   "source": [
    "## 7.2 Visualizing participant's Weekly Summary!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617a9da7",
   "metadata": {},
   "source": [
    "If our participant is curious about a more detailed breakdown of their runs, Strava would show you their weekly summary using the following chart: <br>\n",
    "<img src=\"https://trailingclosure.com/content/images/2020/12/IMG_6409.jpg\" height=\"300\">\n",
    " <br>\n",
    "Again, Let's try to recreate it using the data we have fetched from the Strava API<br><br>\n",
    "<b>Note:</b> Weekly summary includes all the runs, walks and rides walks recorded by the participant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef03022",
   "metadata": {},
   "source": [
    "Enter the dates for the start and end dates for this chart below! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34eb7b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Set date range for the chart above\n",
    "\n",
    "start = \"2022-04-27\" #@param {type:\"date\"}\n",
    "end = \"2022-05-04\" #@param {type:\"date\"}\n",
    "\n",
    "#Function that converts data into a date time object\n",
    "datefixer = lambda date: datetime.fromisoformat(date[0:10])\n",
    "\n",
    "#Applying datefixer function to every column of df_strava_summary\n",
    "df_strava_summary = elevation_df.assign(date \n",
    "                                =elevation_df.get('start_date').apply(datefixer))\n",
    "\n",
    "#Dictionary to store the hourly frequency\n",
    "date_etime = {}\n",
    "\n",
    "#Starting date of our chart\n",
    "start_date = date(int(start.split('-')[0]),\n",
    "                  int(start.split('-')[1]),int(start.split('-')[2]))\n",
    "\n",
    "#Ending date of our chart\n",
    "end_date = date(int(end.split('-')[0]),\n",
    "                int(end.split('-')[1]),int(end.split('-')[2]))\n",
    "\n",
    "#Creating a list of dates between start and end date\n",
    "dates = list(pd.date_range(start_date,end_date-timedelta(days=1),freq='d'))\n",
    "\n",
    "# Storing toal distance, time and elevation\n",
    "total_distance = 0\n",
    "total_time = 0\n",
    "total_elevation = 0\n",
    "\n",
    "for date_val in dates:\n",
    "  date_etime[str(date_val.day)+\" \"+str(date_val.month_name())[:3]] = 0\n",
    "  for actvity_index in range(len(df_strava_summary)):\n",
    "    #Checks if the current date is in the activity DataFrame\n",
    "      if(date_val == df_strava_summary.iloc[actvity_index].get('date')):\n",
    "\n",
    "        #Storing total time, distance and elevation\n",
    "\n",
    "        total_distance = (total_distance +\n",
    "                          df_strava_summary.iloc[actvity_index].get('distance'))\n",
    "        \n",
    "        total_time = (total_time +\n",
    "                      df_strava_summary.iloc[actvity_index].get('moving_time'))\n",
    "        \n",
    "        total_elevation = (total_elevation +\n",
    "              df_strava_summary.iloc[actvity_index].get('total_elevation_gain'))\n",
    "        \n",
    "        #Stores the elevation of moving time in dictionary\n",
    "        date_etime[str(date_val.day)+\" \"\n",
    "        +str(date_val.month_name())[:3]] = (date_etime[str(date_val.day)\n",
    "        +\" \"+str(date_val.month_name())[:3]] +\n",
    "         df_strava_summary.iloc[actvity_index].get('moving_time')/60)\n",
    "\n",
    "#Resetting seaborn to prevent interference with matplotlib plots\n",
    "sns.reset_orig()\n",
    "\n",
    "# Creating a matplotlib plot of size 16,8\n",
    "plt1 = plt.figure(figsize=(16,8))\n",
    "ax = plt1.gca()\n",
    "\n",
    "\n",
    "plt.plot(list(date_etime.keys()),list(date_etime.values()),color=\"#FC5200\",\n",
    "         marker='o', fillstyle='none', lw=3, markerfacecolor='white', ms=20, \n",
    "         mew=3, markeredgecolor='#FC5200')\n",
    "\n",
    "plt.fill_between(list(date_etime.keys()),\n",
    "                 list(date_etime.values()), facecolor='#FC5200',alpha=0.1)\n",
    "\n",
    "\n",
    "\n",
    "# Adding veritcal grids\n",
    "plt.grid(axis=\"x\")\n",
    "\n",
    "# Hiding the y-ticks\n",
    "ax.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "#Adding the title to the chart\n",
    "plt.suptitle(list(date_etime.keys())[0]+\" - \"+\n",
    "             list(date_etime.keys())[-1],fontsize=24,x=0.195)\n",
    "#Adding the subtitle to the chart\n",
    "plt.title( \"$\\\\bf{\"+str(round(total_distance / 1609,1))+\"}$mi  |  \"+\n",
    "          \"$\\\\bf{\"+str(round(total_time/3600))+\"}$h \"+\"$\\\\bf{\"+\n",
    "          str(round(total_time%60))+\"}$m  |  \"+\"$\\\\bf{\"+\n",
    "          str(round(total_elevation))+\"}$ft\",fontsize=18,loc='left')\n",
    "\n",
    "# Removing the spines on top, left and right\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd8c005",
   "metadata": {},
   "source": [
    "<i>Above is a plot we created ourselves!</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be8cea0",
   "metadata": {},
   "source": [
    "## 7.3 Visualizing Participant's Runs!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce720159",
   "metadata": {},
   "source": [
    "Strava's website and app allows the user to visualize the routes for all their activities recorded synced on the Strava application. It is a very interesting feature as it lets the participants view the exact route they took during their activity on an actual map.\n",
    "\n",
    "<img src=\"https://blog.strava.com/wp-content/uploads/2020/03/03-Save-Sync-Share.png\"> <br>\n",
    "\n",
    "\n",
    "Let's try to recreate the plots using the participant's data! <br> <br>\n",
    "Here's a reference to how our overall result will look like\n",
    "\n",
    "<img src=\"https://cf.veloviewer.com/img/veloviewer-strava-segments-homepage.png\" width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03750fa0",
   "metadata": {},
   "source": [
    "Interestingly, Strava provides us with this route data in the form of polylines. In the df_strava dataframe we can see a column called summary_polyline for all the individual activities. We will be using this column for our plot! <br>\n",
    "\n",
    "Polyline is a encoded format that store coordinate location and it has to be decoded before it can be used. If we have a closer look into it, it is a bunch of characters that make no sense to an average reader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d90c263",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polyline\n",
    "import folium\n",
    "\n",
    "#Dictionary to save the coordinates of the first ride\n",
    "my_ride = map_summary_polyline_df.iloc[1].get(['map.summary_polyline']).apply(polyline.decode)['map.summary_polyline']\n",
    "\n",
    "#Select one activity to find the centroid of the map.\n",
    "centroid = [\n",
    "    np.mean([coord[0] for coord in my_ride]), \n",
    "    np.mean([coord[1] for coord in my_ride])\n",
    "  ]\n",
    "\n",
    "#Creating a map \n",
    "m = folium.Map(location=centroid, zoom_start=13)\n",
    "\n",
    "# Plot all rides on map\n",
    "for i in range(len(map_summary_polyline_df.head())):\n",
    "    if len(map_summary_polyline_df.iloc[i].get(['map.summary_polyline'])['map.summary_polyline']) == 0:\n",
    "        continue\n",
    "\n",
    "    # Polyline.decode is a function that helps us decode this polyline data to coordinates\n",
    "    my_ride = map_summary_polyline_df.iloc[i].get(['map.summary_polyline']).apply(polyline.decode)['map.summary_polyline']\n",
    "    # We plot the route on the map.\n",
    "    folium.PolyLine(my_ride, color='red').add_to(m)\n",
    "display(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72662856",
   "metadata": {},
   "source": [
    "# 8. Statistical Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537ccc99",
   "metadata": {},
   "source": [
    "Now that we have found, the weekly summary for the participant's activity time, let's plot some graphs to see if there is a correlation between the various metrics of the participants data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922d9a21",
   "metadata": {},
   "source": [
    "In order to focus on the just the runs, we will have to clean the dataframe and only keep activities of the type 'Run'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbcd807",
   "metadata": {},
   "outputs": [],
   "source": [
    "strava_df = max_speed_df.merge(max_heartrate_df.get(['start_date','max_heartrate']),on='start_date')\n",
    "\n",
    "def filterRuns(data):\n",
    "    dataframe = []\n",
    "    for i in range(len(data)):\n",
    "        name = data.iloc[i].get('name')\n",
    "        if 'run' in name.lower():\n",
    "            dataframe.append(True)\n",
    "        else:\n",
    "            dataframe.append(False)\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "strava_df = strava_df[filterRuns(strava_df)]\n",
    "strava_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d155c0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will drop the null values and only get the columns we need\n",
    "df_runs_cleaned = strava_df.get(['max_speed','max_heartrate']).dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed50a304",
   "metadata": {},
   "source": [
    "As the strava api returns the max_speed values in meters/second and our above plots use miles, we will convert max speed to km/hour to maintain consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7928f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_runs_cleaned = df_runs_cleaned.assign(max_speed =\n",
    "                    df_runs_cleaned.get('max_speed').apply(lambda x: x*3.6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbfab58",
   "metadata": {},
   "source": [
    "Let's plot the cleaned Data Frame below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85938c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_runs_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2820d5",
   "metadata": {},
   "source": [
    "Maybe the heart rate is correlated with how fast you run. Let's test if this hypothesis is true. We will do so by plotting a scatterplot between those two metrics and finding the correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be82bd2",
   "metadata": {},
   "source": [
    "First, we will plot a chart to see if there is a visual correlation between a partcipant's max heart rate and their max speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0abe31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting Figure Size in Seaborn\n",
    "sns.set(rc={'figure.figsize':(16,8)})\n",
    "\n",
    "# Setting Seaborn plot style\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "#Plotting our data\n",
    "plot = sns.regplot(data=df_runs_cleaned, x=\"max_speed\", y=\"max_heartrate\")\n",
    "\n",
    "#Renaming x and y labels\n",
    "plot.set_ylabel(\"Maximum Heart Rate (bpm)\", fontsize = 16)\n",
    "plot.set_xlabel(\"Maximum Speed (km/hr)\", fontsize = 16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43467c5f",
   "metadata": {},
   "source": [
    "As we can see from the scatterplot above, our regression line hints that there might be a correlation between maximum speed and maximum heart rate. Let's compute $R^2$ just to see exactly how correlated.\n",
    "\n",
    "We'll follow [this documentation](https://docs.scipy.org/doc/scipy-0.15.1/reference/generated/scipy.stats.linregress.html) and perform a linear regression to obtain the coefficient of determination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b76987",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(\n",
    "    df_runs_cleaned.get('max_speed'), df_runs_cleaned.get('max_heartrate'))\n",
    "\n",
    "print(f'Slope: {slope:.3g}')\n",
    "print(f'Coefficient of determination: {r_value**2:.3g}')\n",
    "print(f'p-value: {p_value:.3g}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cb6fee",
   "metadata": {},
   "source": [
    "As we can see that the p-value is slightly over 17% which means that there is not enough evidence to convincingly conclude that that there is a correlation between heart rate and speed. Let's try to locate the outliers in this data and find points that might be skewing our dataset and preventing us from gathering enough evidence to prove our correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd9a666",
   "metadata": {},
   "source": [
    "# 9. Outlier Detection and Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511819b5",
   "metadata": {},
   "source": [
    "Before finding the individual outlier values, it would be interesting to see the summary of our max_speed and max_heartrate parameters. It will give us a clear idea of what values are typical and which values can be considered atypical based on the data that we recieved from Strava."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5df96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_runs_cleaned_summary = df_runs_cleaned.describe().get(\n",
    "    ['max_speed','max_heartrate'])\n",
    "df_runs_cleaned_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc81bcf2",
   "metadata": {},
   "source": [
    "To locate the outliers we will be using a supervised as well as unsupervised algorithm called the Elliptic Envelope. In statistical studies, Elliptic Envelope created an imaginary elliptical area around a given dataset where values inside that imaginary area is considered to be normal data, and anything else is assumed to be outliers. It assumes that the given Data follows a gaussian distribution.\n",
    "\n",
    "\"The main idea is to define the shape of the data and anomalies are those observations that lie far outside the shape. First a robust estimate of covariance of data is fitted into an ellipse around the central mode. Then, the Mahalanobis distance that is obtained from this estimate is used to define the threshold for determining outliers or anomalies.\" [(S. Shriram and E. Sivasankar ,2019, pp. 221-225)](https://ieeexplore.ieee.org/document/9004325)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78697539",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.covariance import EllipticEnvelope\n",
    "\n",
    "#create the model, set the contamination as 0.02\n",
    "EE_model = EllipticEnvelope(contamination = 0.02)\n",
    "\n",
    "#implement the model on the data\n",
    "outliers = EE_model.fit_predict(df_runs_cleaned[[\"max_speed\", \"max_heartrate\"]])\n",
    "\n",
    "#extract the labels\n",
    "df_runs_cleaned[\"outlier\"] = outliers\n",
    "\n",
    "#change the labels\n",
    "# We use -1 to mark an outlier and +1 for an inliner\n",
    "df_runs_cleaned[\"outlier\"] = df_runs_cleaned[\"outlier\"].apply(\n",
    "    lambda x: str(-1) if x == -1 else str(1))\n",
    "\n",
    "#extract the score\n",
    "df_runs_cleaned[\"EE_scores\"] = EE_model.score_samples(\n",
    "    df_runs_cleaned[[\"max_speed\", \"max_heartrate\"]])\n",
    "\n",
    "#print the value counts for inlier and outliers\n",
    "print(df_runs_cleaned[\"outlier\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ce0fd8",
   "metadata": {},
   "source": [
    "Below we will replot the df_runs_cleaned dataframe to see how the two new columns were applied to it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f292c728",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_runs_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01de6e15",
   "metadata": {},
   "source": [
    "Now that we have labeled the outliers as -1, let's try to see which values of max heartrate and max speed are being considered as outliers by our Elliptic Envelope Algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62439842",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_df = df_runs_cleaned[df_runs_cleaned.get('outlier')=='-1'].get(\n",
    "    ['max_heartrate','max_speed'])\n",
    "outlier_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29ca36a",
   "metadata": {},
   "source": [
    "Using the dataframe above, we can highlight these outlier values in our original scatterplot in order to visually asses which pair/s of max speed and max heart rate values are not following the general trend seen in our scatterplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef579e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_runs_cleaned.drop(outlier_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cd4290",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setting Figure Size in Seaborn\n",
    "sns.set(rc={'figure.figsize':(16,8)})\n",
    "\n",
    "# Setting Seaborn plot style\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "# Plotting our data\n",
    "# We will calculate the regression line while not accounting for our outliers\n",
    "plot = sns.regplot(data=df_runs_cleaned.drop(outlier_df.index), x=\"max_speed\",\n",
    "                   y=\"max_heartrate\")\n",
    "\n",
    "#Renaming x and y labels\n",
    "plot.set_ylabel(\"Maximum Heart Rate (bpm)\", fontsize = 16)\n",
    "plot.set_xlabel(\"Maximum Speed (km/hr)\", fontsize = 16)\n",
    "\n",
    "# Plotting the outlier and highlighting it\n",
    "plt.scatter(outlier_df.get('max_speed'),outlier_df.get('max_heartrate'))\n",
    "plt.scatter(outlier_df.get('max_speed'),outlier_df.get('max_heartrate'),\n",
    "            facecolors='red',alpha=.35, s=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6eb9511",
   "metadata": {},
   "source": [
    "Thus, the points highlighted in red are ones that seem to not be following the general trend of our dataset. Lastly, let's see what the new p-value is after outlier removal!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d7a7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "slope, intercept, r_value, p_value, std_err = stats.linregress(\n",
    "    df_runs_cleaned.drop(outlier_df.index).get('max_speed'), \n",
    "    df_runs_cleaned.drop(outlier_df.index).get('max_heartrate'))\n",
    "\n",
    "print(f'Slope: {slope:.3g}')\n",
    "print(f'Coefficient of determination: {r_value**2:.3g}')\n",
    "print(f'p-value: {p_value:.3g}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
